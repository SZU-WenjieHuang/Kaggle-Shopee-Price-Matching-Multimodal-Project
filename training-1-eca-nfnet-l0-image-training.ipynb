{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hi all, I hope everyone is doing well.\n# About this notebook \n\nIn this training kernel, I used **'eca_nfnet_l0'(from timm) + ArcMarginProduct Module**. 'eca_nfnet_l0' contains **SiLU() activation**, so I replaced it with **Mish()** activation. Reason to change Mish() activation is beacuse here I am using **Ranger(RAdam + Lookahead)optimizer** and **Mish() + Ranger optimizer** gives a good result (Based on few experiments, I may be wrong). You can try the same strategy to other models too.\n\nYou may find **inference notebook** [here](https://www.kaggle.com/parthdhameliya77/pytorch-eca-nfnet-l0-image-tfidf-inference)\n\n<center><img src=\"https://www.programmersought.com/images/653/8746a02b316eef34dbd8bd83d10ee625.JPEG\"/ width=\"440\" height=\"440\" ></center>\n\n### credits:\n\n[Custom LR schedular](https://www.kaggle.com/tanulsingh077/pytorch-metric-learning-pipeline-only-images?scriptVersionId=58269290&cellId=22) <br>\n[Ranger Optimizer + Centralized Gradient](https://github.com/Yonghongwei/Gradient-Centralization) <br>\n[Mish Activation Function](https://github.com/tyunist/memory_efficient_mish_swish/blob/master/mish.py) <br>","metadata":{}},{"cell_type":"markdown","source":"# Import Packages","metadata":{}},{"cell_type":"code","source":"import sys \nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master') ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 01 导入Library\n\nimport numpy as np \nimport pandas as pd \n\nimport os \nimport cv2 \nimport timm \n\nimport albumentations \nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch \nimport torch.nn.functional as F \nfrom torch import nn \nfrom torch.optim.optimizer import Optimizer\nfrom torch.optim.lr_scheduler import _LRScheduler\n\n# torch.optim.lr_scheduler模块提供了一些根据epoch训练次数来调整学习率（learning rate）的方法。一般情况下我们会设置随着epoch的增大而逐渐减小学习率从而达到更好的训练效果。\n\nimport math\n\nfrom tqdm.notebook import tqdm \n\n# Tqdm 是一个快速，可扩展的Python进度条\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# LabelEncoder 对特征进行硬编码","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config 超参数","metadata":{}},{"cell_type":"code","source":"# Step 02 定义超参数\n\nclass Config:\n    \n    DATA_DIR = '../input/shopee-product-matching/train_images'\n    TRAIN_CSV = '../input/shopee-product-matching/train.csv'\n\n    IMG_SIZE = 512\n    MEAN = [0.485, 0.456, 0.406]\n    STD = [0.229, 0.224, 0.225]\n\n    EPOCHS = 15\n    # Try 15 epochs\n    BATCH_SIZE = 8\n\n    NUM_WORKERS = 4\n    DEVICE = 'cuda'\n\n    CLASSES = 11014 \n    SCALE = 30 \n    MARGIN = 0.5\n\n    MODEL_NAME = 'eca_nfnet_l1'\n    FC_DIM = 512\n    SCHEDULER_PARAMS = {\n            \"lr_start\": 1e-5,\n            \"lr_max\": 1e-5 * 32,\n            \"lr_min\": 1e-6,\n            \"lr_ramp_ep\": 5,\n            \"lr_sus_ep\": 0,\n            \"lr_decay\": 0.8,\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"# Step 03 构造一个class对数据集的初始定义\n\nclass ShopeeDataset(torch.utils.data.Dataset):\n\n    ##定义输入端 实例化数据\n    \n    def __init__(self,df, transform = None): \n        self.df = df \n        self.root_dir = Config.DATA_DIR\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self,idx):\n\n        row = self.df.iloc[idx]\n\n        img_path = os.path.join(self.root_dir,row.image)\n        print(img_path)\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        label = row.label_group\n\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n\n        return {\n            'image' : image,\n            'label' : torch.tensor(label).long()\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Augs 图像增强","metadata":{}},{"cell_type":"code","source":"# Step 04 def一个图像增强函数\n\ndef get_train_transforms():\n    return albumentations.Compose(\n        [   \n            albumentations.Resize(Config.IMG_SIZE,Config.IMG_SIZE,always_apply=True),\n            albumentations.HorizontalFlip(p=0.5),\n            albumentations.VerticalFlip(p=0.5),\n            albumentations.Rotate(limit=120, p=0.8),\n            albumentations.RandomBrightness(limit=(0.09, 0.6), p=0.5),\n            albumentations.Normalize(mean = Config.MEAN, std = Config.STD),\n            ToTensorV2(p=1.0),\n        ]\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**pytorch LearningRate 的调整方法总结**\n\n原文链接：https://blog.csdn.net/chaipp0607/article/details/112986446","metadata":{}},{"cell_type":"code","source":"# Step 05 定义一个ShopeeSchedulerClass  来调整Learning Rate 学习率调节\n\n#credit : https://www.kaggle.com/tanulsingh077/pytorch-metric-learning-pipeline-only-images?scriptVersionId=58269290&cellId=22\n\n\nclass ShopeeScheduler(_LRScheduler):\n    \n    #定义输入端 实例化数据\n    \n    def __init__(self, optimizer, lr_start=5e-6, lr_max=1e-5,\n                 lr_min=1e-6, lr_ramp_ep=5, lr_sus_ep=0, lr_decay=0.8,\n                 last_epoch=-1):\n        self.lr_start = lr_start\n        self.lr_max = lr_max\n        self.lr_min = lr_min\n        self.lr_ramp_ep = lr_ramp_ep\n        self.lr_sus_ep = lr_sus_ep\n        self.lr_decay = lr_decay\n        super(ShopeeScheduler, self).__init__(optimizer, last_epoch)\n        \n    #获取最新的学习率\n    \n    def get_lr(self): \n        if not self._get_lr_called_within_step:\n            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n                          \"please use `get_last_lr()`.\", UserWarning)\n        \n        if self.last_epoch == 0:\n            self.last_epoch += 1\n            return [self.lr_start for _ in self.optimizer.param_groups]\n        \n        lr = self._compute_lr_from_epoch()\n        self.last_epoch += 1\n        \n        return [lr for _ in self.optimizer.param_groups]\n    \n    def _get_closed_form_lr(self):\n        return self.base_lrs\n    \n    def _compute_lr_from_epoch(self):\n        if self.last_epoch < self.lr_ramp_ep:\n            lr = ((self.lr_max - self.lr_start) / \n                  self.lr_ramp_ep * self.last_epoch + \n                  self.lr_start)\n        \n        elif self.last_epoch < self.lr_ramp_ep + self.lr_sus_ep:\n            lr = self.lr_max\n            \n        else:\n            lr = ((self.lr_max - self.lr_min) * self.lr_decay**\n                  (self.last_epoch - self.lr_ramp_ep - self.lr_sus_ep) + \n                  self.lr_min)\n        return lr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Gradient-Centralization**\n\n梯度集中 一种新的梯度优化方法\n\n梯度集中（GC）是用于深度神经网络（DNN）的一种简单有效的优化技术，该技术通过将梯度向量集中为零均值来直接对梯度进行操作。它既可以加速训练过程，又可以提高DNN的最终泛化性能。","metadata":{}},{"cell_type":"markdown","source":"# Ranger优化器\n\nRanger 优化器结合了两个非常新的发展(RAdam + Lookahead)到一个单一的优化器中","metadata":{}},{"cell_type":"code","source":"# Step 06 定义优化器得到Loss\n\n#credit : https://github.com/Yonghongwei/Gradient-Centralization\n\n#定义Gradient-Centralization\n\ndef centralized_gradient(x, use_gc=True, gc_conv_only=False):\n    if use_gc:\n        if gc_conv_only:\n            if len(list(x.size())) > 3:\n                x.add_(-x.mean(dim=tuple(range(1, len(list(x.size())))), keepdim=True))\n        else:\n            if len(list(x.size())) > 1:\n                x.add_(-x.mean(dim=tuple(range(1, len(list(x.size())))), keepdim=True))\n    return x\n\n\n#定义Ranger优化器\n\nclass Ranger(Optimizer): \n\n    def __init__(self, params, lr=1e-3,                       # lr\n                 alpha=0.5, k=5, N_sma_threshhold=5,           # Ranger options\n                 betas=(.95, 0.999), eps=1e-5, weight_decay=0,  # Adam options\n                 # Gradient centralization on or off, applied to conv layers only or conv + fc layers\n                 use_gc=True, gc_conv_only=False, gc_loc=True\n                 ):\n\n        # parameter checks\n        if not 0.0 <= alpha <= 1.0:\n            raise ValueError(f'Invalid slow update rate: {alpha}')\n        if not 1 <= k:\n            raise ValueError(f'Invalid lookahead steps: {k}')\n        if not lr > 0:\n            raise ValueError(f'Invalid Learning Rate: {lr}')\n        if not eps > 0:\n            raise ValueError(f'Invalid eps: {eps}')\n\n        # parameter comments:\n        # beta1 (momentum) of .95 seems to work better than .90...\n        # N_sma_threshold of 5 seems better in testing than 4.\n        # In both cases, worth testing on your dataset (.90 vs .95, 4 vs 5) to make sure which works best for you.\n\n        # prep defaults and init torch.optim base\n        defaults = dict(lr=lr, alpha=alpha, k=k, step_counter=0, betas=betas,\n                        N_sma_threshhold=N_sma_threshhold, eps=eps, weight_decay=weight_decay)\n        super().__init__(params, defaults)\n\n        # adjustable threshold\n        self.N_sma_threshhold = N_sma_threshhold\n\n        # look ahead params\n\n        self.alpha = alpha\n        self.k = k\n\n        # radam buffer for state\n        self.radam_buffer = [[None, None, None] for ind in range(10)]\n\n        # gc on or off\n        self.gc_loc = gc_loc\n        self.use_gc = use_gc\n        self.gc_conv_only = gc_conv_only\n        # level of gradient centralization\n        #self.gc_gradient_threshold = 3 if gc_conv_only else 1\n\n        print(\n            f\"Ranger optimizer loaded. \\nGradient Centralization usage = {self.use_gc}\")\n        if (self.use_gc and self.gc_conv_only == False):\n            print(f\"GC applied to both conv and fc layers\")\n        elif (self.use_gc and self.gc_conv_only == True):\n            print(f\"GC applied to conv layers only\")\n\n    def __setstate__(self, state):\n        print(\"set state called\")\n        super(Ranger, self).__setstate__(state)\n\n    def step(self, closure=None):\n        loss = None\n        # note - below is commented out b/c I have other work that passes back the loss as a float, and thus not a callable closure.\n        # Uncomment if you need to use the actual closure...\n\n        # if closure is not None:\n        #loss = closure()\n\n        # Evaluate averages and grad, update param tensors\n        for group in self.param_groups:\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        'Ranger optimizer does not support sparse gradients')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]  # get state dict for this param\n\n                if len(state) == 0:  # if first time to run...init dictionary with our desired entries\n                    # if self.first_run_check==0:\n                    # self.first_run_check=1\n                    #print(\"Initializing slow buffer...should not see this at load from saved model!\")\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n\n                    # look ahead weight storage now in state dict\n                    state['slow_buffer'] = torch.empty_like(p.data)\n                    state['slow_buffer'].copy_(p.data)\n\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(\n                        p_data_fp32)\n\n                # begin computations\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                # GC operation for Conv layers and FC layers\n                # if grad.dim() > self.gc_gradient_threshold:\n                #    grad.add_(-grad.mean(dim=tuple(range(1, grad.dim())), keepdim=True))\n                if self.gc_loc:\n                    grad = centralized_gradient(grad, use_gc=self.use_gc, gc_conv_only=self.gc_conv_only)\n\n                state['step'] += 1\n\n                # compute variance mov avg\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n\n                # compute mean moving avg\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n\n                buffered = self.radam_buffer[int(state['step'] % 10)]\n\n                if state['step'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state['step']\n                    beta2_t = beta2 ** state['step']\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * \\\n                        state['step'] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n                    if N_sma > self.N_sma_threshhold:\n                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (\n                            N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n                    else:\n                        step_size = 1.0 / (1 - beta1 ** state['step'])\n                    buffered[2] = step_size\n\n                # if group['weight_decay'] != 0:\n                #    p_data_fp32.add_(-group['weight_decay']\n                #                     * group['lr'], p_data_fp32)\n\n                # apply lr\n                if N_sma > self.N_sma_threshhold:\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    G_grad = exp_avg / denom\n                else:\n                    G_grad = exp_avg\n\n                if group['weight_decay'] != 0:\n                    G_grad.add_(p_data_fp32, alpha=group['weight_decay'])\n                # GC operation\n                if self.gc_loc == False:\n                    G_grad = centralized_gradient(G_grad, use_gc=self.use_gc, gc_conv_only=self.gc_conv_only)\n\n                p_data_fp32.add_(G_grad, alpha=-step_size * group['lr'])\n                p.data.copy_(p_data_fp32)\n\n                # integrated look ahead...\n                # we do it at the param level instead of group level\n                if state['step'] % group['k'] == 0:\n                    # get access to slow param tensor\n                    slow_p = state['slow_buffer']\n                    # (fast weights - slow weights) * alpha\n                    slow_p.add_(p.data - slow_p, alpha=self.alpha)\n                    # copy interpolated weights to RAdam param tensor\n                    p.data.copy_(slow_p)\n\n        return loss","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 06 修改激活函数 用Mish激活\n\n#credit : https://github.com/tyunist/memory_efficient_mish_swish/blob/master/mish.py\n\n''' I just wanted to understand and implement custom backward activation in PyTorch so I choose this.\n    You can also simply use this function below too.\n\nclass Mish(nn.Module):\n    def __init__(self):\n        super(Mish, self).__init__()\n\n    def forward(self, input):\n        return input * (torch.tanh(F.softplus(input)))\n'''\n\nclass Mish_func(torch.autograd.Function):\n    \n    @staticmethod\n    def forward(ctx, i):\n        result = i * torch.tanh(F.softplus(i))\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_tensors[0]\n  \n        v = 1. + i.exp()\n        h = v.log() \n        grad_gh = 1./h.cosh().pow_(2) \n\n        # Note that grad_hv * grad_vx = sigmoid(x)\n        #grad_hv = 1./v  \n        #grad_vx = i.exp()\n        \n        grad_hx = i.sigmoid()\n\n        grad_gx = grad_gh *  grad_hx #grad_hv * grad_vx \n        \n        grad_f =  torch.tanh(F.softplus(i)) + i * grad_gx \n        \n        return grad_output * grad_f \n\n\nclass Mish(nn.Module):\n    def __init__(self, **kwargs):\n        super().__init__()\n        print(\"Mish initialized\")\n        pass\n    def forward(self, input_tensor):\n        return Mish_func.apply(input_tensor)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 06 修改激活函数\n\ndef replace_activations(model, existing_layer, new_layer):\n    for name, module in reversed(model._modules.items()):\n        if len(list(module.children())) > 0:\n            model._modules[name] = replace_activations(module, existing_layer, new_layer)\n\n        if type(module) == existing_layer:\n            layer_old = module\n            layer_new = new_layer\n            model._modules[name] = layer_new\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Model ","metadata":{}},{"cell_type":"code","source":"# Step 07 定义ArcMargin损失函数\n\nclass ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.scale\n\n        return output, nn.CrossEntropyLoss()(output,label)\n\n    \n# Step 08 定义最终模型ShopeeModel\n\nclass ShopeeModel(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = Config.CLASSES,\n        model_name = Config.MODEL_NAME,\n        fc_dim = Config.FC_DIM,\n        margin = Config.MARGIN,\n        scale = Config.SCALE,\n        use_fc = True,\n        pretrained = True):\n\n\n        super(ShopeeModel,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n        \n        #定义基础网络结构 backbone\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n\n        if model_name == 'resnext50_32x4d':\n            final_in_features = self.backbone.fc.in_features\n            self.backbone.fc = nn.Identity() #自定义全连接层 Full-Connected\n            self.backbone.global_pool = nn.Identity() #自定义池化层pooling\n\n        elif 'efficientnet' in model_name:\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity() #自定义分类层\n            self.backbone.global_pool = nn.Identity() #自定义池化层pooling\n        \n        elif 'nfnet' in model_name:\n            final_in_features = self.backbone.head.fc.in_features\n            self.backbone.head.fc = nn.Identity() #自定义全连接层 Full-Connected\n            self.backbone.head.global_pool = nn.Identity() #自定义池化层pooling\n\n        #自适应平均池化 只需要给定输出特征图的大小就好，其中通道数前后不发生变化\n        \n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n\n        self.use_fc = use_fc\n\n        #定义反向传播优化\n        \n        if use_fc:\n            self.dropout = nn.Dropout(p=0.0)\n            self.fc = nn.Linear(final_in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            final_in_features = fc_dim\n\n        self.final = ArcMarginProduct(\n            final_in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n\n    def _init_params(self): #初始化参数\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, image, label): #前向传播\n        feature = self.extract_feat(image)\n        logits = self.final(feature,label)\n        return logits\n\n    def extract_feat(self, x): #特征提取\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.fc(x)\n            x = self.bn(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef train_fn(model, data_loader, optimizer, scheduler, i):\n    \n    #data_loader数据加载器\n    \n    model.train()\n    fin_loss = 0.0\n    tk = tqdm(data_loader, desc = \"Epoch\" + \" [TRAIN] \" + str(i+1))\n\n    for t,data in enumerate(tk):\n        for k,v in data.items():\n            data[k] = v.to(Config.DEVICE)\n        optimizer.zero_grad()\n        _, loss = model(**data)\n        loss.backward()\n        optimizer.step() \n        fin_loss += loss.item() \n\n        tk.set_postfix({'loss' : '%.6f' %float(fin_loss/(t+1)), 'LR' : optimizer.param_groups[0]['lr']})\n\n    scheduler.step()\n\n    return fin_loss / len(data_loader)\n\ndef eval_fn(model, data_loader, i):\n    \n    #evaluate 评价\n    \n    model.eval()\n    fin_loss = 0.0\n    tk = tqdm(data_loader, desc = \"Epoch\" + \" [VALID] \" + str(i+1))\n\n    with torch.no_grad():\n        for t,data in enumerate(tk):\n            for k,v in data.items():\n                data[k] = v.to(Config.DEVICE)\n            _, loss = model(**data)\n            fin_loss += loss.item() \n\n            tk.set_postfix({'loss' : '%.6f' %float(fin_loss/(t+1))})\n        return fin_loss / len(data_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def run_training():\n    \n    #读取训练文件\n    \n    df = pd.read_csv(Config.TRAIN_CSV)\n\n    #给特征进行编码\n    labelencoder= LabelEncoder()\n    df['label_group'] = labelencoder.fit_transform(df['label_group'])\n    \n    # 用我们定义的ShopeeDataset Class对数据集预处理\n    trainset = ShopeeDataset(df, transform = get_train_transforms())\n\n    trainloader = torch.utils.data.DataLoader(\n        trainset,\n        batch_size = Config.BATCH_SIZE,\n        pin_memory = True,\n        num_workers = Config.NUM_WORKERS,\n        shuffle = True,\n        drop_last = True\n    )\n    \n    #用我们上面定义好的ShopeeModel\n    \n    model = ShopeeModel()\n    model.to(Config.DEVICE)\n    \n    \n    existing_layer = torch.nn.SiLU\n    \n    # 把eca_nfnet_l0的激活函数SiLU()变成Mish\n    \n    new_layer = Mish()\n    model = replace_activations(model, existing_layer, new_layer) # in eca_nfnet_l0 SiLU() is used, but it will be replace by Mish()\n    \n    # 用我们定义的Ranger优化器来优化梯度\n    \n    optimizer = Ranger(model.parameters(), lr = Config.SCHEDULER_PARAMS['lr_start'])\n    \n     # 用我们定义的ShopeeScheduler Class来调整学习率\n        \n    scheduler = ShopeeScheduler(optimizer,**Config.SCHEDULER_PARAMS) #\n    \n    # 保存权重文件\n\n    RESUME = False\n    \n    if RESUME:\n        path_checkpoint = \"./model_parameter/test/ckpt_best_50.pth\"  # 断点路径\n        checkpoint = torch.load(path_checkpoint)  # 加载断点\n \n        model.load_state_dict(checkpoint['net'])  # 加载模型可学习参数\n \n        optimizer.load_state_dict(checkpoint['optimizer'])  # 加载优化器参数\n        start_epoch = checkpoint['epoch']  # 设置开始的epoch\n        lr_schedule.load_state_dict(checkpoint['lr_schedule'])\n\n    for epoch in range(Config.EPOCHS):\n    \n        avg_loss_train = train_fn(model, trainloader, optimizer, scheduler, epoch)\n        checkpoint = {\n        \"net\": model.state_dict(),\n        'optimizer':optimizer.state_dict(),\n        \"epoch\": epoch,\n        #'scheduler':scheduler.state_dict()\n        }\n\n        \n        torch.save(checkpoint, './ckpt_best_%s.pth' %(str(epoch)))\n        \n        \n\nrun_training()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]}]}